# Flink
## 1. Flink 简介
### Flink 是什么？
* Apache Flink 是一个**框架**和**分布式**处理引擎，用于对**无界和有界数据流**进行**状态**计算



### 为什么选择 Flink？
* 传统的数据架构只能处理固定的数据，无法不断添加新数据计算
* 目标：
    - 低延迟
    - 高吞吐
    - 结果的准确性和良好的容错性



### 哪些行业需要处理流数据
* 对实时性要求比较高的
* 电商和市场营销
    - 数据报表、广告投放、业务流程需要
* 物联网（IOT）
    - 传感器实时数据采集和显示、实时报警，交通运输业
* 电信业
    - 基站流量调配
* 银行和金融业
    - 实时结算和通知推送，实时检测异常行为



### Flink 的主要特点
* 事件驱动
* 基于流的世界观
    - 在 Flink 的世界观中，一切都是由流组成的，离线数据是有界的流，实时数据是无界的流
* 分层 API
    - 越顶层越抽象，表达含义越简明，使用越灵活
    - 越底层越具体，表达能力越丰富，使用越灵活
* 其他特点
    - 支持事件时间（event-time）和处理时间（processing-time）语义
    - 精确一次（exactly-once）的状态一致性保证
    - 低延迟，每秒处理数百万个事件，毫秒级延迟
    - 与众多常用存储系统的连接
    - 高可用，动态扩展，实现 7 * 24 小时全天候运行




### Flink vs Spark Streaming
* 数据模型
    - spark 采用 RDD 模型，spark streaming 的 DStream 实际上也就是一组组小批数据 RDD 的集合
    - flink 基本数据模型是数据流，以及事件（Event）序列
* 运行时架构
    - spark 是批计算，将 DAG 划分为不同的 stage，一个完成后才可以计算下一个
    - flink 是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节点进行处理




### 事件驱动型应用
* 事件驱动型应用是一类具有**状态**的应用，它从一个或多个事件流提取数据，并根据到来的事件触发计算、状态更新或其他外部动作
* 事件驱动型应用是在计算存储分离的传统应用基础上进化而来。在传统架构中，应用需要读写远程事务性数据库
* 事件驱动型应用是基于状态化流处理来完成。在该设计中，数据和计算不会分离，应用只需访问本地（内存或磁盘）即可获取数据。系统容错性的实现依赖于定期向远程持久化存储写入checkpoint
* 优势
    - 事件驱动型应用无须查询远程数据库，本地数据访问使得它具有更高的吞吐和更低的延迟。而由于定期向远程持久化存储的 checkpoint 工作可以异步、增量式完成，因此对于正常事件处理的影响甚微。
    - 事件驱动型应用的优势不仅限于本地数据访问。传统分层架构下，通常多个应用会共享同一个数据库，因而任何对数据库自身的更改（如由应用更新或服务扩容导致数据布局发生改变）都需要谨慎协调。而事件驱动型应用，由于只需考虑自身数据，因此在更改数据表示或服务扩容时所需的协调工作将大大减少。




### 数据分析应用
* 数据分析任务需要从原始数据中提取有价值的信息和指标。传统的分析方式通常是利用批查询，或将事件记录下来并给予此有限数据集构建应用来完成。为了得到最新数据的分析结果，必须先将它们加入分析数据集并重新执行查询或运行应用，随后将结果写入存储系统或生成报告
* 借助一些先进的流处理引擎，还可以实时地进行数据分析。和传统模式下读取有限数据集不同，流式查询或应用汇接入实时事件流，并随着事件消费持续产生和更新结果。这些结果数据可能会写入外部数据库系统或以内部状态的形式维护。仪表展示应用可以相应地从外部数据库读取数据或直接查询应用的内部状态
* **流式分析应用的优势**
    - 和批量分析相比，由于流式分析省掉了周期性的数据导入和查询过程，因此从事件中获取指标的延迟更低。不仅如此，批量查询必须处理那些由定期导入和输入有界性导致的人工数据边界，而流式查询则无需考虑该问题
    - 流式分析会简化应用抽象。批量查询的流水线通常由多个独立部件组成，需要周期性地调度提取数据和执行查询。如此复杂的流水线操作起来并不容易，一旦某个组件出错将会影响流水线的后续步骤。而流式分析应用整体运行在 Flink 之类的高端流处理系统之上，涵盖了从数据接入到连续结果计算的所有步骤，因此可以依赖底层引擎提供的故障恢复机制




### 数据管道
* 提取-转换-加载（ETL）是一种在存储系统之间进行数据转换和迁移的常用方法。ETL 作业通常会周期性地触发，将数据从事务型数据库拷贝到分析型数据库或数据仓库
* 数据管道和 ETL 作业的用途相似，都可以转换、丰富数据，并将其从某个存储系统移动到另一个。但数据管道是以持续流模式运行，而非周期性触发。因此它支持从一个不断生成数据的源头读取记录，并将它们以低延迟移动到终点。例如：数据管道可以用来监控文件系统目录中的新文件，并将其数据写入事件日志；另一个应用可能会将事件流物化到数据库或增量构建和优化查询索引
* 优势
    - 和周期性 ETL 作业相比，持续数据管道可以明显降低将数据移动到目的端的延迟。此外，由于它能够持续消费和发送数据，因此用途更广，支持用例更多




## 2. 流处理基础
### 数据并行和任务并行
* 数据并行：将输入数据分组，让同一操作的多个任务并行执行在不同数据子集上。能够将计算负载分配到多个节点上从而允许处理大规模的数据
* 任务并行：让不同算子的任务（基于相同或不同的数据）并行计算。可以更好地利用集群的计算资源




### 数据交换策略
* 转发策略：在发送端任务和接收端任务之间一对一地进行数据传输。如果两端任务运行在同一物理机器上，该交换策略可以避免网络通信
* 广播策略：把每个数据项发往下游算子的全部并行任务。该策略会把数据复制多份且涉及网络通信，因此代价十分昂贵
* 基于键值的策略：根据某一键值属性对数据分区，并保证键值相同的数据项会交由同一任务处理
* 随机策略：将数据均匀分配至算子的所有任务，以实现计算任务的负载均衡




### 延迟和吞吐
* 延迟：处理一个事件所需的时间
* 吞吐：用来衡量系统处理能力（处理速率）的指标，告诉我们系统每单位时间可以处理多少事件
* 降低延迟实际上可以提高吞吐。系统执行操作越快，相同时间内执行的操作数目就会越多。通过并行处理多条数据流，可以在处理更多事件的同时降低延迟




### 窗口
* 转换操作和滚动聚合每次处理一个事件来产生输出并（可能）更新状态。然而，有些操作必须收集并缓冲记录才能计算结果，例如流式 Join 或像是求中位数的整体聚合。为了在在无限数据流上高效地执行这些操作，必须对操作所维持的数据量加以限制。除此之外，窗口操作还支持在数据流上完成一些具有切实语义价值的查询。
* 窗口操作会持续创建一些称为“桶”的有限事件集合，并允许我们基于这些有限集进行计算。事件通常会根据其事件或其他数据属性分配到不同桶中。为了准确定义窗口算子语义，我们需要决定事件如何分配到桶中以及窗口用怎样的频率产生结果
* 窗口类型语义
    - 滚动窗口：将事件分配到长度固定且互不重叠的桶中。在窗口边界通过后，所有事件会发送给计算函数进行处理
    - 滑动窗口：将事件分配到大小固定且允许相互重叠的桶中，意味着每个事件可能会同时属于多个桶
    - 会话窗口：根据会话间隔将事件分为不同的会话，该间隔值定义了会话在关闭前的非活动时间长度




### 时间语义
* 处理时间：当前流处理算子所在机器上的本地时钟时间。基于处理时间的窗口会包含那些恰好在一段时间内到达窗口算子的事件，这里的时间段是按照机器时间测量的
* 事件时间：数据流中事件实际发生的时间，它以附加在数据流中事件的时间戳为依据。这些时间戳通常在事件数据进入流处理管道之前就存在。即使事件有延迟，事件时间窗口也能准确地将事件分配到窗口中，从而反映出真实发生的情况
* 比较：
    - 处理时间窗口能够将延迟降至最低。由于无需考虑迟到或乱序的事件，窗口只需简单地缓冲事件，然后在达到特定事件后立即触发窗口计算即可。因此对于那些更重视处理速度而非准确度的应用，处理事件就能派上用场
    - 如果你需要周期性地实时报告结果而无论其准确定如何，处理时间窗口能够表示数据流自身的真实情况。
    - 虽然处理时间提供了很低的延迟，但它的结果依赖处理速度，具有不确定性
    - 事件时间能保证结果的准确定，并允许你处理延迟甚至无序的事件，但延迟较高
* 水位线
    - 水位线是一个全局进度指标，表示我们确信不会再有延迟事件到来的某个时间点
    - 本质上，水位线提供了一个逻辑时钟，用来通知系统当前的事件时间。当一个算子接收到时间为 T 的水位线，就可以认为不会再收到任何时间戳小于或等于 T 的事件了
    - 水位线无论对于事件时间窗口还是处理乱序事件的算子都很关键。算子一旦收到某个水位线，就相当于接到信号：某个特定时间区间的时间戳已经到齐，可以触发窗口计算或对接收的数据进行排序了




### 状态和一致性模型
* 为了生成结果，函数会在一段时间或基于一定个数的事件来累积状态。有状态算子同时使用传入的事件和内部状态来计算输出
* 一致性模型
    - 至多一次：任务发生故障时既不恢复丢失的状态，也不重放丢失的事件，保证每个事件至多被处理一次。事件可以随意丢弃，没有任何机制来保证结果的正确性
    - 至少一次：所有事件最终都会处理，虽然有些可能会处理多次。如果正确定仅依赖信息的完整度，那重复处理或许可以接受。为了确保至少一次结果语义的正确性，需要想办法从源头或缓冲区中重放事件。持久化事件日志会将所有事件写入永久存储，这样在任务故障时就可以重放它们
    - 精确一次：表示不但没有事件丢失，而且每个事件对于内部状态的更新都只有一次。本质上，精确一次保障意味着应用总会提供正确的结果，就如同故障从未发生过一般。提供精确一次保障是以至少一次保障为前提，因此同样需要数据重放机制
    - 端到端的精确一次：在整个数据处理管道上结果都是正确的。在每个组件都提供自身的保障情况下，整个处理管道上端到端的保障会受制于保障最弱的那个组件




## 3. Flink 运行架构
### Flink 运行时的组件
#### 任务提交流程（宏观）
![任务提交流程（宏观）](https://cdn.jsdelivr.net/gh/qtxsnwwb/image-hosting@master/20210522/任务提交流程（宏观）.3jcp3ce2j3w0.png)

#### 1. 作业管理器（JobManager）
* 控制一个应用程序执行的主进程，也就是说，每个应用程序都会被一个不同的 JobManager 所控制执行
* JobManager 会先接收到要执行的应用程序，这个应用程序会包括：作业图（JobGraph）、逻辑数据流图（logical dataflow graph）和打包了所有的类、库和其他资源的 JAR 包
* JobManager 会把 JobGraph 转换成一个物理层面的数据流图，这个图被叫做“执行图”（ExecutionGraph），包含了所有可以并发执行的任务
* JobManager 会向资源管理器（ResourceManager）请求执行任务必要的资源，也就是任务管理器（TaskManager）上的插槽（slot）。一旦获取到了足够的资源，就会将执行图分发到真正运行它们的 TaskManager 上。而在运行过程中，JobManager 会负责所有需要中央协调的操作，比如说检查点（checkpoints）的协调



#### 2. 任务管理器（TaskManager）
* Flink 中的工作进程。通常在 Flink 中会有多个 TaskManager 运行，每一个 TaskManager 都包含了一定数量的插槽（slots）。插槽的数量限制了 TaskManager 能够执行的任务数量
* 启动之后，TaskManager 会向资源管理器注册它的插槽；收到资源管理器的指令后，TaskManager 就会将一个或者多个插槽提供给 JobManager 调用。JobManager 就可以向插槽分配任务（tasks）来执行了
* 在执行过程中，一个 TaskManager 可以跟其他运行同一应用程序的 TaskManager 交换数据



#### 3. 资源管理器（ResourceManager）
* 主要负责管理任务管理器（TaskManager）的插槽（slot），TaskManager 插槽是 Flink 中定义的处理资源单元
* Flink 为不同的环境和资源管理工具提供了不同资源管理器，比如 YARN、Mesos、K8s，以及 standalone 部署
* 当 JobManager 申请插槽资源时，ResourceManager 会将有空闲插槽的 TaskManager 分配给 JobManager。如果 ResourceManager 没有足够的插槽来满足 JobManager 的请求，它还可以向资源提供平台发起会话，以提供启动 TaskManager 进程的容器



#### 4. 分发器（Dispatcher）
* 可以跨作业运行，它为应用提交提供了 REST 接口
* 当一个应用被提交执行时，分发器就会启动并将应用移交给一个 JobManager
* Dispatcher 也会启动一个 Web UI，用来方便地展示和监控作业执行的信息
* Dispatcher 在架构中可能并不是必需的，这取决于应用提交运行的方式



### 任务提交流程（YARN）
![任务提交流程（YARN）](https://cdn.jsdelivr.net/gh/qtxsnwwb/image-hosting@master/20210522/任务提交流程（YARN）.7khan19egc80.png)

### 任务调度原理
![任务调度原理](https://cdn.jsdelivr.net/gh/qtxsnwwb/image-hosting@master/20210522/任务调度原理.46fxhgzyz4o0.png)




### 并行度
* 一个特定算子的子任务（subtask）的个数被称之为其并行度（parallelism）
* 一般情况下，一个 stream 的并行度，可以认为就是其所有算子中最大的并行度



### TaskManager 和 Slots
* Flink 中每一个 TaskManager 都是一个 JVM 进程，它可能会在独立的线程上执行一个或多个子任务
* **为了控制一个 TaskManager 能接收多少个 task**，TaskManager 通过 task slot 来进行控制（一个 TaskManager 至少有一个 slot）
* 默认情况下，Flink 允许子任务共享 slot，即使它们是不同任务的子任务。这样的结果是，一个 slot 可以保存作业的整个管道
* Task Slot 是静态的概念，是指 TaskManager 具有的并发执行能力



### 程序与数据流（DataFlow）
* 所有的 Flink 程序都是由三部分组成的：Source、Transformation 和 Sink
* Source 负责读取数据源，Transformation 利用各种算子进行处理加工，Sink 负责输出
* 在运行时，Flink 上运行的程序会被映射成“逻辑数据流”（dataflows），它包含了这三部分
* 每一个 dataflow 以一个或多个 sources 开始，以一个或多个 sinks 结束。dataflow 类似于任意的有向无环图（DAG）
* 在大部分情况下，程序中的转换运算（transformations）跟 dataflow 中的算子（operator）是一一对应的关系



### 执行图（ExecutionGraph）
* Flink 中的执行图可以分成四层：StreamGraph -> JobGraph -> ExecutionGraph -> 物理执行图
* StreamGraph：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构
* JobGraph：StreamGraph 经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点
* ExecutionGraph：JobManager 根据 JobGraph 生成 ExecutionGraph。ExecutionGraph 是 JobGraph 的并行化版本，是调度层最核心的数据结构
* 物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个 TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构




### 数据传输形式
* 一个程序中，不同的算子可能具有不同的并行度
* 算子之间传输数据的形式可以是 one-to-one（forwarding）的模式也可以是 redistributing 的模式，具体是哪一种形式，取决于算子的种类
* One-to-one：stream 维护着分区以及元素的顺序（比如 source 和 map 之间）。这意味着 map 算子的子任务看到的元素的个数以及顺序跟 source 算子的子任务生产的元素的个数、顺序相同。map、filter、flatMap 等算子都是 one-to-one 的对应关系
* Redistributing：stream 的分区会发生改变。每一个算子的子任务依据所选择的 transformation 发送数据到不同的目标任务。例如，keyBy 基于 hashCode 重分区、而 broadcast 和 rebalance 会随机重新分区，这些算子都会引起 redistribute 过程，而 redistribute 过程就类似于 Spark 中的 shuffle 过程

#### Flink 数据传输
* TaskManager 负责将数据从发送任务传输至接收任务。它的网络模块在记录传输前会先将它们收集到缓冲区中。即记录并非逐个发送，而是在缓冲区中以批次形式发送
* 每个 TaskManager 都有一个用于收发数据的网络缓冲池（默认 32 KB）。如果发送端和接收端的任务运行在不同的 TaskManager 进程中，它们就要用到操作系统的**网络栈**进行通信。流式应用需要以**流水线**方式交换数据，因此每对 TaskManager 之间都要维护一个或多个**永久的 TCP 连接**来执行数据交换。对于每一个接收任务，TaskManager 都要提供一个专用的网络缓冲区，用于接收其他任务发来的数据

#### 基于信用值的流量控制
* 通过网络连接逐条发送记录不但低效，还会导致很多额外开销
* 基于信用值的流量控制：接收任务会给发送任务授予一定的信用值，其实就是保留一些用来接收它数据的网络缓冲。一旦发送端收到信用通知，就会在信用值所限定的范围内尽可能多地传输缓冲数据，并会附带上积压量（已经填满准备传输的网络缓冲数目）大小。接收端使用保留的缓冲来处理收到的数据，同时依据各发送端的积压量信息来计算所有相连的发送端在下一轮的信用优先级
* 由于发送端可以在接收端有足够资源时立即传输数据，所以基于信用值的流量控制可以有效降低延迟



### 任务链（Operator Chains）
* Flink 采用了一种称为任务链的优化技术，可以在特定条件下减少本地通信的开销。为了满足任务链的要求，必须将两个或多个算子设为相同的并行度，并通过本地转发（local forward）的方式进行连接
* 条件：
    - 多个算子拥有相同的并行度
    - 通过本地转发通道相连
* **相同并行度**的 **one-to-one** 操作，Flink 这样相连的算子链接在一起形成一个 task，原来的算子成为里面的 subtask
* 并行度相同、并且是 ont-to-one 操作，两个条件缺一不可




### 高可用性设置
#### TaskManager 故障
* 假设一个 Flink 设置包含了 4 个 TaskManager，每个 TaskManager 有 2 个处理槽，那么一个流式应用最多支持以并行度 8 来运行。如果有一个 TaskManager 出现故障，则可用处理槽的数量就降到了 6 个。这时候 JobManager 就会向 ResourceManager 申请更多的处理槽。若无法完成，JobManager 将无法重启应用，直至有足够数量的可用处理槽。

#### JobManager 故障
* 支持在原 JobManager 消失的情况下将作业的管理职责及元数据迁移到另一个 JobManager
* Flink 的高可用模式是基于能够提供分布式协调和共识服务的 Apache ZooKeeper 来完成的，它在 Flink 中主要用于“领导”选举以及持久且高可用的数据存储
* JobManager 会将 JobGraph 以及全部所需的元数据（例如应用的 JAR 文件）写入一个远程持久化存储系统。此外，JobManager 会将存储位置的路径地址写入 ZooKeeper 的数据存储。在应用执行过程中，JobManager会接收每个任务检查点的状态句柄（存储位置）。在检查点即将完成的时候，如果所有任务已经将各自状态成功写入远程存储，JobManager 就会将状态句柄写入远程存储，并将远程位置的路径地址写入 ZooKeeper。因此所有用于 JobManager 故障恢复的数据都在远程存储上面，而 ZooKeeper 持有这些存储位置的路径




### 事件时间处理
#### 时间戳
* Flink 流式应用处理的所有记录都必须包含时间戳。时间戳将记录和特定时间点进行关联，这些时间点通常是记录所对应事件的发生时间。但实际上应用可以自由选择时间戳的含义，只要保证流记录的时间戳会随着数据流的前进大致递增即可。
* 当 Flink 以事件时间模式处理数据流时，会根据记录的时间戳触发时间相关算子的计算
* Flink 内部此阿勇 8 字节的 Long 值对时间戳进行编码，并将它们以元数据（metadata）的形式附加在记录上，内置算子会将这个 Long 值解析为毫秒精度的 Unix 时间戳

#### 水位线
* 水位线（watermark）用于在事件时间应用中推断每个任务当前的事件时间。基于时间的算子会使用这个时间来触发计算并推动进度前进
* Flink 中水位线是利用一些包含 Long 值时间戳的特殊记录来实现的
* 基本属性：
    - 必须单调递增。这是为了确保任务中的事件时间时钟正确前进，不会倒退
    - 和记录的时间戳存在联系。一个时间戳为 T 的水位线表示，接下来所有记录的时间戳一定都大于 T
* 水位线的意义之一在于它允许控制结果的完整性和延迟。如果水位线和记录的时间戳非常接近，那结果的处理延迟就会很低，因为任务无须等待过多记录就可以触发最终计算。但同时结果的完整性可能会受影响，因为可能有部分相关记录被视为迟到记录，没能参与运算。水位线会增加处理延迟，但同时结果的完整性也会有所提升

#### 水位线传播和事件时间
* Flink 内部将水位线实现为特殊的记录，它们通过算子任务进行接收和发送。任务内部的时间服务会维护一些计时器，它们依靠接收到水位线来激活。这些计时器是由任务在时间服务内注册，并在将来的某个时间点执行计算。
* 当任务接收到一个水位线时会执行以下操作：
    1. 基于水位线记录的时间戳更新内部事件时间时钟
    2. 任务的时间服务会找出所有触发时间小于更新后事件时间的计时器。对于每个到期的计时器，调用回调函数，利用它来执行计算或发出记录
    3. 任务根据更新后的事件时间将水位线发出
* 一个任务会为它的每个输入分区都维护一个分区水位线。当收到某个分区传来的水位线后，任务会以接收值和当前值中较大的那个去更新对应分区水位线的值。随后，任务会把事件时间时钟调整为所有分区水位线中最小的那个值。如果事件时间时钟向前推动，任务会先处理因此而触发的所有计时器，之后才会把对应的水位线发往所有连接的输出分区，以实现事件时间到全部下游任务的广播。
![水位线传播](https://cdn.jsdelivr.net/gh/qtxsnwwb/image-hosting@master/20210522/水位线传播.2k6g75wtga40.png)

#### 时间戳分配和水位线生成
* 时间戳和水位线通常都是在数据流刚刚进入流处理应用的时候分配和生成的。由于不同的应用会选择不同的时间戳，而水位线依赖于时间戳和数据流本身的特征，所以应用必须显式地分配时间戳和生成水位线。
* Flink DataStream 应用可以通过三种方式完成该工作：
    - 在数据源完成
    - 周期分配器
    - 定点分配器




### 状态管理
* 函数里所有需要任务去维护并用来计算结果的数据都属于任务的状态，可以把状态想象成任务的业务逻辑所需要访问的本地或实例变量
![状态管理](https://cdn.jsdelivr.net/gh/qtxsnwwb/image-hosting@master/20210522/状态管理.6rzp03s8ar40.png)

#### 算子状态
* 算子状态的作用范围限定为算子任务，由同一并行任务所处理的所有数据都可以访问到相同的状态
* 状态对于同一子任务而言是共享的
* 算子状态不能由相同或不同算子的另一个子任务访问
* 算子状态数据结构：
    - 列表状态（List state）：将状态表示为一组数据的列表
    - 联合列表状态（Union list state）：也将状态表示为数据的列表。它与常规列表状态的区别在于，在发生故障时，或者从保存点（savepoint）启动应用程序时如何恢复
    - 广播状态（Broadcast state）：如果一个算子有多项任务，而它的每项任务状态又都相同，那么这种特殊情况最适合应用广播状态
    ![算子状态](https://cdn.jsdelivr.net/gh/qtxsnwwb/image-hosting@master/20210522/算子状态.a5wp7mixkog.png)

#### 键值分区状态
* 键值分区状态是根据输入数据流中定义的键（key）来维护和访问的
* Flink 为每个 key 维护一个状态实例，并将具有相同键的所有数据，都分区到同一个算子任务中，这个任务会维护和处理这个 key 对应的状态
* 当任务处理一条数据时，它会自动将状态的访问范围限定为当前数据的 key
* 键值分区状态数据结构：
    - 值状态（Value state）：将状态表示为单个的值
    - 列表状态（List state）：将状态表示为一组数据的列表
    - 映射状态（Map state）：将状态表示为一组 Key-Value 对
    - 聚合状态（Reducing state & Aggregating state）：将状态表示为一个用于聚合操作的列表
    ![键值分区状态](https://cdn.jsdelivr.net/gh/qtxsnwwb/image-hosting@master/20210522/键值分区状态.yjincd3gk74.png)

#### 状态后端
* 为了保证快速访问状态，每个并行任务都会把状态维护在本地。状态具体的存储、访问和维护是由一个名为状态后端的**可插拔组件**来决定。
* 状态后端主要负责两件事：**本地状态管理**和**将状态以检查点的形式写入远程存储**
    - 对于本地状态管理，状态后端会存储所有键值分区状态，并保证能将状态访问范围正确地限制在当前键值。Flink 提供的一类状态后端会把键值分区状态作为对象，以内存数据结构的形式存在 JVM 堆中
    - 另一类状态后端会把状态对象序列化后存到 RocksDB 中，RocksDB 负责将它们写到本地磁盘上
    - 前者状态访问更快一些，但会受到内存大小的限制；后者状态访问会慢一些，但允许状态变得很大
* 状态后端负责将任务状态以检查点形式写入远程持久化存储，该远程存储可能是一个分布式文件系统，也可能是某个数据库系统

#### 有状态算子的扩缩容
* 对于有状态算子，该变并行度会复杂很多，因为我们需要把状态重新分组，分配到与之前数量不等的并行任务上
* Flink 对不同类型的状态提供了四种扩缩容模式
    - **带有键值分区状态的算子在扩缩容时会根据新的任务数量对键值重新分区**。为了降低状态在不同任务之间迁移的必要成本，Flink 不会对单独的键值实施再分配，而是会把所有键值分为不同的键值组（key group）。每个键值组都包含了部分键值，Flink 以此为单位把键值分配给不同任务。
    - **带有算子列表状态的算子在扩缩容时会对列表中的条目进行重新分配**。所有并行算子任务的列表条目会被统一收集起来，随后均匀分配到更少或更多的任务之上。如果列表条目的数量小于算子新设置的并行度，部分任务在启动时的状态就可能为空。
    - **带有算子联合列表状态的算子会在扩缩容时把状态列表的全部条目广播到全部任务上**。随后由任务自己决定哪些条目该保留，哪些该丢弃。
    - **带有算子广播状态的算子在扩缩容时会把状态拷贝到全部新任务上**。这样做的原因是广播状态能确保所有任务的状态相同。在缩容的情况下，由于状态经过复制不会丢失，我们可以简单地停掉多出的任务。




### 检查点
#### 一致性检查点
* Flink 的故障恢复机制需要基于应用状态的一致性检查点
* 有状态的流式应用的一致性检查点是在所有任务处理完等量的原始输入后对全部任务状态进行的一个拷贝

#### 从一致性检查点中恢复
* 在流式应用执行过程中，Flink 会周期性地为应用状态生成检查点。一旦发生故障，Flink 会利用最新的检查点将应用状态恢复到某个一致性的点并重启处理进程
* 应用恢复步骤：
    1. 重启整个应用
    2. 利用最新的检查点重置任务状态
    3. 恢复所有任务的处理
* 如果所有算子都将它们全部的状态写入检查点并从中恢复，并且所有输入流的消费位置都能重置到检查点生成那一刻，那么该检查点和恢复机制就能为整个应用的状态提供**精确一次**的一致性保障

#### Flink 检查点算法
* Flink 的检查点是基于 Chandy-Lamport 分布式快照算法实现的。该算法不会暂停整个应用，而是会把**生成检查点的过程和处理过程分离**，这样在部分任务持久化状态的过程中，其他任务还可以继续执行
* Flink 的检查点算法中会用到一类名为检查点分隔符（checkpoint barrier）的特殊记录。和水位线类似，这些检查点分隔符会通过数据源算子注入到常规的记录流中。相对其他记录，它们在流中的位置无法提前或延后。为了标识所属的检查点，每个检查点分隔符都会带有一个检查点编号，这样就把一条数据流从逻辑上分成了两个部分。所有先于分隔符的记录所引起的状态更改都会被包含在分隔符所对应的检查点之中，而所有晚于分隔符的记录所引起的状态更改都会被纳入之后的检查点中

#### 检查点对性能的影响
* 任务在将其状态存入检查点的过程中，会处于阻塞状态，此时的输入会进入缓冲区。按照 Flink 的设计，是由状态后端负责生成检查点，因此任务的状态的具体拷贝过程完全取决于状态后端的实现
* 我们还可以对分隔符对齐这一步进行调整，以降低检查点算法对处理延迟的影响。对于那些需要极低延迟且能容忍至少一次状态保障的应用，可以通过配置让 Flink 在分隔符对齐的过程中不缓冲那些已收到分隔符所对应分区的记录，而是直接处理它们。待所有的检查点分隔符都到达以后，算子才将状态存入检查点，这时候状态可能会包含一些由本应出现在下一次检查点的记录所引起的改动。一旦出现故障，这些记录会被重复处理，而这意味着检查点只能提供至少一次而非精确一次的一致性保障




### 保存点
* 保存点的生成算法和检查点完全一样，因此可以把保存点看做包含一些额外元数据的检查点
* 保存点的生成不是由 Flink 自动完成，而是需要由用户（或外部调度器）显式触发。同时，Flink 也不会自动清理保存点

#### 保存点的使用
* 给定一个应用和一个兼容的保存点，我们可以从该保存点启动应用。相比于检查点，将应用从某个保存点启动还能做更多事情：
    - 从保存点启动一个不同但相互兼容的应用。这意味着你可以修复应用的一些逻辑 bug，然后在数据流来源的支持范围内下尽可能多地重新处理输入事件，以此来修复结果。应用修改还可用于 A/B 测试或需要不同业务逻辑的假想场景。应用和保存点必须相互兼容，只有这样应用才能加载保存点内的状态
    - 用不同的并行度启动原应用，从而实现应用的扩缩容
    - 在另一个集群上启动相同的应用。这允许你把应用迁移到一个新的 Flink 版本，或是一个不同的集群或数据中心
    - 利用保存点暂停某个应用，稍后再把它启动起来。这样可以为更高优先级的应用腾出集群资源，或者在输入数据不连续的情况下及时释放资源
    - 为保存点设置不同版本并将应用状态归档